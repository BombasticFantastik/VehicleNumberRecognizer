{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2140244-896d-4913-b19c-1e4636a660f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.nn import Module\n",
    "from torch.optim import AdamW\n",
    "import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1708940-86ef-4434-9d57-d2f00f6ae4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet=[symb for symb in '_ABEKMHOPCTYX0123456789']\n",
    "let2int={i:let for let,i in enumerate(alphabet)}\n",
    "int2let={let:i for let,i in enumerate(alphabet)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40684b55-02d4-4e8c-a2dc-e45b2be9c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumberDataset(Dataset):\n",
    "    def __init__(self,path,number_len):\n",
    "        super(NumberDataset,self).__init__()\n",
    "        self.number_len=number_len\n",
    "        img_path=os.path.join(path,'img')\n",
    "        label_path=os.path.join(path,'ann')\n",
    "\n",
    "        #номера\n",
    "        self.image_numbers=[img[:-4] for img in os.listdir(img_path)]\n",
    "        self.label_numbers=[label[:-5] for label in os.listdir(label_path)]\n",
    "        \n",
    "        #изображения и лейблы \n",
    "        self.images=[os.path.join(img_path,img) for img in os.listdir(img_path) if img[:-4] in self.label_numbers]\n",
    "        self.labels=[os.path.join(label_path,label) for label in os.listdir(label_path) if label[:-5] in self.image_numbers]\n",
    "        \n",
    "        self.images.sort(reverse=True)\n",
    "        self.labels.sort(reverse=True)\n",
    "\n",
    "        self.trans=transforms.Compose([\n",
    "            transforms.Resize((64,128)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    def __getitem__(self,idx):\n",
    "        idx_img=Image.open(self.images[idx]).convert('RGB')\n",
    "        idx_label=self.labels[idx]\n",
    "        print(self.labels[idx])\n",
    "        with open(idx_label,'r') as file_option:\n",
    "            jf=json.load(file_option)\n",
    "            tensor_label=torch.tensor([let2int[let] for let in jf['name'][0:self.number_len]])\n",
    "        tensor_img=self.trans(idx_img)\n",
    "        print(len(tensor_label))\n",
    "        return {\n",
    "            'img':tensor_img,\n",
    "            'label':tensor_label,\n",
    "            'label_len':len(idx_label)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93e616e3-483e-4aee-975b-03139c5775bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imgs = torch.stack([x['img'] for x in batch])\n",
    "    labels=[x['label'] for x in batch]\n",
    "    label_lens=torch.tensor([x['label_len'] for x in batch])\n",
    "    label=nn.utils.rnn.pad_sequence(labels,batch_first=True,padding_value=0)\n",
    "    return imgs,label,label_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "edd233e0-ae53-4b31-a983-7b97298d9841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//home/artemybombastic/MyGit/VehicleNumberData/VNR_Data/train/ann/Y999YY51.json\n",
      "8\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 1 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m plt.imshow(\u001b[43mnumber_data\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mRuntimeError\u001b[39m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 1 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "plt.imshow(number_data[0]['label'].detach().permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c1f76b4-ae8f-43da-83e8-2820dd0172de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//home/artemybombastic/MyGit/VehicleNumberData/VNR_Data/train/ann/Y999YY51.json\n",
      "8\n",
      "Y\n",
      "9\n",
      "9\n",
      "9\n",
      "Y\n",
      "Y\n",
      "5\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in number_data[0]['label']:\n",
    "    print(int2let[i.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b5138205-ae20-4b6e-9e97-3b70230259a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_data=NumberDataset('//home/artemybombastic/MyGit/VehicleNumberData/VNR_Data/train',9)\n",
    "number_dataloader=DataLoader(number_data,batch_size=16,shuffle=False,drop_last=True,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d051014-db7d-4436-bf76-2683ba539928",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(Module):\n",
    "    def __init__(self,input_size,output_size,stride=1,downsample=None):#downsample нужно в случае понижения размерности блока):\n",
    "        super().__init__()\n",
    "        self.act=nn.ReLU(inplace=True)\n",
    "        self.conv0=nn.Conv2d(input_size,output_size,kernel_size=3,stride=stride,padding=1)\n",
    "        self.norm0=nn.BatchNorm2d(output_size)\n",
    "\n",
    "        self.conv1=nn.Conv2d(output_size,output_size,kernel_size=3,stride=1,padding=1)\n",
    "        self.norm1=nn.BatchNorm2d(output_size)\n",
    "\n",
    "        self.downsample=downsample\n",
    "    def forward(self,x):\n",
    "        out=self.conv0(x)\n",
    "        out=self.norm0(out)\n",
    "        out=self.act(out)\n",
    "        out=self.conv1(out)\n",
    "        out=self.norm1(out)\n",
    "        if self.downsample:\n",
    "            x=self.downsample(x)\n",
    "        out+=x\n",
    "        out=self.act(out)\n",
    "\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19b680e8-e5d8-4d19-a80d-68eb91c3c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layers(block,cnt,input_size,output_size,stride=1,downsample=False):\n",
    "    blocks=[]\n",
    "\n",
    "    if downsample or input_size!=output_size or stride!=1:\n",
    "        downsample=nn.Sequential(\n",
    "            nn.Conv2d(input_size,output_size,1,stride,bias=False),\n",
    "            nn.BatchNorm2d(output_size)\n",
    "        )\n",
    "\n",
    "    blocks.append(block(input_size,output_size,stride,downsample))    \n",
    "    for i in range(1,cnt):\n",
    "        blocks.append(block(output_size,output_size))\n",
    "\n",
    "    return nn.Sequential(*blocks)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "056a09ab-7353-42ea-97a0-bc796a7ff75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet34(Module):\n",
    "    def __init__(self,input_size,hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.initial_lay=nn.Sequential(\n",
    "            nn.Conv2d(input_size,hidden_size,kernel_size=7,stride=2,padding=3),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "        )\n",
    "\n",
    "        self.lay0=make_layers(block=ResnetBlock,cnt=3,input_size=hidden_size,output_size=hidden_size,downsample=False)\n",
    "        self.lay1=make_layers(block=ResnetBlock,cnt=4,input_size=hidden_size,output_size=hidden_size*2,stride=2,downsample=True)\n",
    "        self.lay2=make_layers(block=ResnetBlock,cnt=6,input_size=hidden_size*2,output_size=hidden_size*4,stride=2,downsample=True)\n",
    "        self.lay3=make_layers(block=ResnetBlock,cnt=3,input_size=hidden_size*4,output_size=hidden_size*8,stride=2,downsample=True)\n",
    "\n",
    "        self.avg_pool=nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        print(x.shape)\n",
    "        out=self.initial_lay(x)\n",
    "        print(out.shape)\n",
    "        print('СЛОИ:')\n",
    "        out=self.lay0(out)\n",
    "        print(out.shape)\n",
    "        out=self.lay1(out)\n",
    "        print(out.shape)\n",
    "        out=self.lay2(out)\n",
    "        print(out.shape)\n",
    "        out=self.lay3(out)\n",
    "        print(out.shape)\n",
    "\n",
    "        final_out=self.avg_pool(out)\n",
    "        print(final_out.shape)\n",
    "        return final_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bf5ce6c-f237-46ea-8476-0ace72ec65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=nn.CTCLoss()\n",
    "model=Resnet34(input_size=3,hidden_size=64)\n",
    "optimizer=AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a499fe2a-7de0-4a0d-a49f-8924863ebe53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "266ad887-e744-455d-a395-3871c3ed97cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "80\n",
      "80\n",
      "79\n",
      "79\n",
      "80\n",
      "80\n",
      "81\n",
      "79\n",
      "81\n",
      "79\n",
      "80\n",
      "80\n",
      "79\n",
      "80\n",
      "80\n",
      "torch.Size([16, 3, 64, 128])\n",
      "torch.Size([16, 64, 16, 32])\n",
      "СЛОИ:\n",
      "torch.Size([16, 64, 16, 32])\n",
      "torch.Size([16, 128, 8, 16])\n",
      "torch.Size([16, 256, 4, 8])\n",
      "torch.Size([16, 512, 2, 4])\n",
      "torch.Size([16, 512, 1, 1])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "CTCLoss.forward() missing 2 required positional arguments: 'input_lengths' and 'target_lengths'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m img,label,label_len=batch\n\u001b[32m      4\u001b[39m pred=model(img)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m loss=\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: CTCLoss.forward() missing 2 required positional arguments: 'input_lengths' and 'target_lengths'"
     ]
    }
   ],
   "source": [
    "for batch in number_dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    img,label,label_len=batch\n",
    "    pred=model(img)\n",
    "\n",
    "    loss=loss_fn(pred,label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "361d0535-921b-451c-887f-9b851b2566cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([79, 80, 80, 79, 79, 80, 80, 81, 79, 81, 79, 80, 80, 79, 80, 80])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce9623c9-35c3-4105-a7fc-a534fe1413c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label)*9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612d0c7a-3fbb-4420-9110-2937d88752fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
